{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sahi import AutoDetectionModel\n",
    "from ultralytics.utils import metrics\n",
    "\n",
    "from utils import plot_bboxes, detection_metrics, convert_yolo_to_torch, sliced_inference, load_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test P&ID Image Dimensions\n",
    "IMAGE_WIDTH = 2339\n",
    "IMAGE_HEIGHT = 1653\n",
    "X1 = 100\n",
    "X2 = 1779\n",
    "Y1 = 200\n",
    "Y2 = 1403\n",
    "CONFIDENCE_THRESHOLD = 0.75\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "root = Path().resolve().parents[1]\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "HEIGHT = IMAGE_HEIGHT - Y1 - (IMAGE_HEIGHT - Y2)\n",
    "WIDTH = IMAGE_WIDTH - X1 - (IMAGE_WIDTH - X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training P&ID Dimensions\n",
    "IMAGE_WIDTH = 5268\n",
    "IMAGE_HEIGHT = 4011\n",
    "X1 = 100\n",
    "X2 = 1779\n",
    "Y1 = 200\n",
    "Y2 = 1403\n",
    "CONFIDENCE_THRESHOLD = 0.75\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "root = Path().resolve().parents[1]\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = root / 'Data' / 'test' / 'fw_system' / 'fw_system-1_cropped.jpg'\n",
    "im = cv2.imread(im_path)\n",
    "test_labels = root / 'Data' / 'test' / 'fw_system' / 'labels' / '1de1eea3-fw_system-1_cropped.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract GT Images from P&ID using Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor = load_labels(test_labels)\n",
    "label_tensor_sorted = torch.stack(sorted(label_tensor, key=lambda label_tensor: label_tensor[0]))\n",
    "\n",
    "unique, idx = label_tensor_sorted[:, 0].unique(dim=0, sorted=True, return_inverse=True)\n",
    "\n",
    "# Get 'ground truth' label by taking first instance of each unique class\n",
    "unique_indices = torch.cat(\n",
    "    [\n",
    "        torch.ones(1), # Base case\n",
    "        torch.tensor([i for i in range(1, idx.size(0)) if idx[i] != idx[i-1]]) # Sliding window \n",
    "    ]\n",
    ").long()\n",
    "\n",
    "gt_labels = label_tensor_sorted[unique_indices]\n",
    "class_ids = gt_labels[:, 0].long()\n",
    "gt_labels_torch = convert_yolo_to_torch(gt_labels, height=HEIGHT, width=WIDTH, x1=X1, y1=Y1, cropped=False)\n",
    "\n",
    "# Save Extracted Images\n",
    "for gt_label, class_id in zip(gt_labels_torch, class_ids):\n",
    "    \n",
    "    id = class_id.item()\n",
    "    x1, x2 = torch.round(gt_label[::2]).int()\n",
    "    y1, y2 = torch.round(gt_label[1::2]).int()\n",
    "\n",
    "    write_path = root / 'Data' / 'test' / 'fw_system' / 'labels' / f'class_{id}.png'\n",
    "    if not cv2.imwrite(write_path, im[y1:y2, x1:x2]):\n",
    "        print('Failed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract images from detected Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIDObjectLookupTable import ObjectLookupTable\n",
    "from PIDDetectionEncoder import ObjectEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 48 slices.\n"
     ]
    }
   ],
   "source": [
    "model_folder = 'train8'\n",
    "model_path = f'runs/detect/{model_folder}/weights/best.pt'\n",
    "im_path = root / 'Data' / 'test' / 'fw_system' / 'fw_system-1_cropped.jpg'\n",
    "im = cv2.imread(im_path)\n",
    "\n",
    "model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=model_path,\n",
    "    confidence_threshold = CONFIDENCE_THRESHOLD,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "results = sliced_inference(model, img_path=str(im_path), slice_height=256, slice_width=256, h_ratio=0.2, w_ratio=0.2) \n",
    "results.export_visuals(export_dir=root, hide_labels=True)\n",
    "pred_bboxes = torch.tensor([x.bbox.to_xyxy() for x in results.object_prediction_list])\n",
    "\n",
    "for i, detection_bbox in enumerate(pred_bboxes):\n",
    "    \n",
    "    x1, x2 = torch.round(detection_bbox[::2]).int()\n",
    "    y1, y2 = torch.round(detection_bbox[1::2]).int()\n",
    "\n",
    "    write_path = root / 'Data' / 'test' / 'fw_system' / 'predictions' / f'{model_folder}' /f'prediction_{i+1}.png'\n",
    "    if not cv2.imwrite(write_path, im[y1:y2, x1:x2]):\n",
    "        print('Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT Class Images\n",
    "gt_imgs_path = root / 'Data' / 'test' / 'fw_system' / 'labels' / '*.png'\n",
    "gt_imgs = glob.glob(str(gt_imgs_path))\n",
    "\n",
    "# Detection Images\n",
    "imgs_path = root / 'Data' / 'test' / 'fw_system' / 'predictions' / model_folder / '*.png'\n",
    "prediction_imgs = glob.glob(str(imgs_path))\n",
    "\n",
    "# Class ID to Labels\n",
    "class_labels = root / 'Data' / 'test' / 'fw_system' / 'labels' / 'notes.json'\n",
    "with open(class_labels) as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "# Translate Class ID to labels\n",
    "gt_indices = [0, 1, 3, 6, 7, 9, 11, 12, 16, 18, 21, 23, 24, 25, 27, 28, 30, 31, 32, 39]\n",
    "labels = [d['categories'][i] for i in gt_indices]\n",
    "\n",
    "# Pre-Trained Encoder Path\n",
    "encoder_pth = root / 'nyu-capstone-2024-PIDGraph' / 'Object Detection'/ 'models' / 'encoder_20250414.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'name': 'Ball Valve N.C.'},\n",
       " {'id': 1, 'name': 'Ball Valve N.O.'},\n",
       " {'id': 3, 'name': 'Centrifugal Pump'},\n",
       " {'id': 6, 'name': 'Continuation Arrow'},\n",
       " {'id': 7, 'name': 'Control Measurement Point'},\n",
       " {'id': 9, 'name': 'Coupling'},\n",
       " {'id': 11, 'name': 'Flange'},\n",
       " {'id': 12, 'name': 'Flow Arrow'},\n",
       " {'id': 16, 'name': 'Gate Valve N.O. #2'},\n",
       " {'id': 18, 'name': 'Gate Valve Pneumatic'},\n",
       " {'id': 21, 'name': 'Globe Valve N.O.'},\n",
       " {'id': 23, 'name': 'Hose Bib'},\n",
       " {'id': 24, 'name': 'Inline Flow Meter'},\n",
       " {'id': 25, 'name': 'Inline Indicator'},\n",
       " {'id': 27, 'name': 'Motor'},\n",
       " {'id': 28, 'name': 'Needle Valve'},\n",
       " {'id': 30, 'name': 'PLC'},\n",
       " {'id': 31, 'name': 'Pressure Vessel'},\n",
       " {'id': 32, 'name': 'Reducer'},\n",
       " {'id': 39, 'name': 'Swinging Disk Check Valve'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIDDetectionEncoder import LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Images Embeddings & Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew Deur\\miniconda3\\envs\\capstone\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = torch.load(encoder_pth)\n",
    "lookup_tables = ObjectLookupTable(labels, gt_imgs, encoder, img_size=224)\n",
    "max_sim, match_class = lookup_tables.classify(prediction_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atlas Embedding Map of Training Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nomic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnomic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m atlas\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m gt_imgs_path = root / \u001b[33m'\u001b[39m\u001b[33mData\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33mSimCLR\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33m*.png\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nomic'"
     ]
    }
   ],
   "source": [
    "from nomic import atlas\n",
    "import numpy as np\n",
    "\n",
    "gt_imgs_path = root / 'Data' / 'SimCLR' / '*.png'\n",
    "gt_imgs = glob.glob(str(gt_imgs_path))\n",
    "\n",
    "encoder = torch.load(encoder_pth)\n",
    "lookup_tables = ObjectLookupTable(labels, gt_imgs, encoder, img_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = lookup_tables.gt_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = root / 'nyu-capstone-2024-PIDGraph' / 'Object Detection' / 'classes_general.json'\n",
    "with open(class_labels) as json_data:\n",
    "    d = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      2\u001b[39m img_classes = [\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(re.findall(\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+)(?:_[a-z])?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m, path)[\u001b[32m0\u001b[39m]))\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m gt_imgs\n\u001b[32m      5\u001b[39m     ]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m text_labels = \u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclass_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg_classes\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      2\u001b[39m img_classes = [\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(re.findall(\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+)(?:_[a-z])?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m, path)[\u001b[32m0\u001b[39m]))\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m gt_imgs\n\u001b[32m      5\u001b[39m     ]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m text_labels = [\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mclass_name\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m img_classes]\n",
      "\u001b[31mKeyError\u001b[39m: '0'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "img_classes = [\n",
    "    str(int(re.findall('(\\d+)(?:_[a-z])?\\.png', path)[0]))\n",
    "    for path in gt_imgs\n",
    "    ]\n",
    "text_labels = [d[i]['class_name'] for i in img_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = \u001b[43m[\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgt_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m data = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mtext_labels\u001b[49m[i],\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage_path\u001b[39m\u001b[33m\"\u001b[39m: img_path\n\u001b[32m      6\u001b[39m     }\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gt_imgs)\n\u001b[32m      8\u001b[39m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'text_labels' is not defined"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"id\": f\"img_{i}\",\n",
    "        \"label\": text_labels[i],\n",
    "        \"image_path\": img_path\n",
    "    }\n",
    "    for i, img_path in enumerate(gt_imgs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'atlas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43matlas\u001b[49m.map_data(\n\u001b[32m      2\u001b[39m     data=data,\n\u001b[32m      3\u001b[39m     blobs=gt_imgs,\n\u001b[32m      4\u001b[39m     embeddings=embeddings_np,\n\u001b[32m      5\u001b[39m     identifier=\u001b[33m'\u001b[39m\u001b[33mP&ID Object Embeddings (Resnet18 Backbone, Low Temp)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'atlas' is not defined"
     ]
    }
   ],
   "source": [
    "atlas.map_data(\n",
    "    data=data,\n",
    "    blobs=gt_imgs,\n",
    "    embeddings=embeddings_np,\n",
    "    identifier='P&ID Object Embeddings (Resnet18 Backbone, Low Temp)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def calculate_mean_std(image_paths, target_size=(224, 224)):\n",
    "    # Initialize lists to store pixel values for each channel\n",
    "    red_values = []\n",
    "    green_values = []\n",
    "    blue_values = []\n",
    "\n",
    "    # Loop through all image paths\n",
    "    for img_path in image_paths:\n",
    "        # Open the image\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")  # Ensure the image is in RGB mode\n",
    "        \n",
    "        # Resize the image to the target size\n",
    "        img = img.resize(target_size)\n",
    "        \n",
    "        # Convert the image to a numpy array\n",
    "        img_array = np.array(img)\n",
    "\n",
    "        # Append pixel values of each channel\n",
    "        red_values.append(img_array[:, :, 0])\n",
    "        green_values.append(img_array[:, :, 1])\n",
    "        blue_values.append(img_array[:, :, 2])\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    red_values = np.concatenate(red_values)\n",
    "    green_values = np.concatenate(green_values)\n",
    "    blue_values = np.concatenate(blue_values)\n",
    "\n",
    "    # Calculate mean and std for each channel\n",
    "    red_mean, red_std = np.mean(red_values), np.std(red_values)\n",
    "    green_mean, green_std = np.mean(green_values), np.std(green_values)\n",
    "    blue_mean, blue_std = np.mean(blue_values), np.std(blue_values)\n",
    "\n",
    "    return (red_mean, red_std), (green_mean, green_std), (blue_mean, blue_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stats = \u001b[43mcalculate_mean_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_imgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mcalculate_mean_std\u001b[39m\u001b[34m(image_paths, target_size)\u001b[39m\n\u001b[32m     16\u001b[39m img = img.resize(target_size)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Convert the image to a numpy array\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m img_array = \u001b[43mnp\u001b[49m.array(img)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Append pixel values of each channel\u001b[39;00m\n\u001b[32m     22\u001b[39m red_values.append(img_array[:, :, \u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "stats = calculate_mean_std(gt_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstats\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'stats' is not defined"
     ]
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roestta Stone between Class / Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_ids = []\n",
    "\n",
    "from collections import namedtuple\n",
    "Classification = namedtuple('Classification', ['id', 'name'])\n",
    "\n",
    "for _match in match_class:\n",
    "    id = lookup_tables.class_labels[_match.item()]['id']\n",
    "    name = lookup_tables.class_labels[_match.item()]['name']\n",
    "    c = Classification(id, name)\n",
    "    detection_ids.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_labels) \u001b[38;5;28;01mas\u001b[39;00m jl:\n\u001b[32m      4\u001b[39m     labels_conversion = json.load(jl)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m labels_adjusted = \u001b[43mdf\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      8\u001b[39m classes = labels_adjusted.apply(\u001b[38;5;28;01mlambda\u001b[39;00m row: labels_conversion[\u001b[33m'\u001b[39m\u001b[33mcategories\u001b[39m\u001b[33m'\u001b[39m][row][\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m classes\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "json_labels = root / 'Data' / 'test' / 'fw_system' / 'labels' / 'notes.json'\n",
    "\n",
    "with open(json_labels) as jl:\n",
    "    labels_conversion = json.load(jl)\n",
    "\n",
    "labels_adjusted = df[0]\n",
    "\n",
    "classes = labels_adjusted.apply(lambda row: labels_conversion['categories'][row]['name'])\n",
    "classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m class_counts_detection = Counter([item.name \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m detection_ids])\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Extract class names and counts from the 'classes' series\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m class_counts_series = Counter(\u001b[43mclasses\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Extract class names and counts (the union of keys)\u001b[39;00m\n\u001b[32m      9\u001b[39m names = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(class_counts_detection.keys()).union(class_counts_series.keys()))\n",
      "\u001b[31mNameError\u001b[39m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "class_counts_detection = Counter([item.name for item in detection_ids])\n",
    "\n",
    "# Extract class names and counts from the 'classes' series\n",
    "class_counts_series = Counter(classes)\n",
    "\n",
    "# Extract class names and counts (the union of keys)\n",
    "names = list(set(class_counts_detection.keys()).union(class_counts_series.keys()))\n",
    "\n",
    "# Get counts for each class from both sources, defaulting to 0 if not found\n",
    "detection_values = [class_counts_detection.get(name, 0) for name in names]\n",
    "series_values = [class_counts_series.get(name, 0) for name in names]\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(names, detection_values, color='skyblue', label='Matched Classes')\n",
    "plt.bar(names, series_values, bottom=detection_values, color='orange', label='Ground Truth Classes')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Class Name')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Matched Classes Distribution vs Ground Truth')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 606.6666,  292.3619,  651.7353,  337.4257],\n",
       "        [ 606.8184,  222.0527,  651.7449,  266.7242],\n",
       "        [1137.0739,  868.9871, 1180.5472,  913.4901],\n",
       "        [ 561.8498,  997.5872,  606.7565, 1042.2794],\n",
       "        [ 618.9312,  997.6917,  664.0825, 1042.5157],\n",
       "        [1080.1989,  868.8502, 1123.7912,  914.2368],\n",
       "        [  75.1580,  953.4601,  120.5877,  998.2258],\n",
       "        [ 183.2225,  887.7995,  227.2415,  934.7491],\n",
       "        [ 590.5762,  751.4611,  634.5347,  799.7556],\n",
       "        [1345.8783,  759.3105, 1389.2747,  804.4855],\n",
       "        [ 182.8255,  953.0795,  228.0563,  997.8292],\n",
       "        [1081.1742,  622.6985, 1126.8682,  667.9127],\n",
       "        [  75.7868,  889.4388,  119.7221,  934.1135],\n",
       "        [ 562.0662,  906.4601,  605.8758,  950.6668],\n",
       "        [ 524.4335,  199.3483,  569.5709,  243.7855],\n",
       "        [ 682.2962,  744.0904,  726.2284,  789.0168],\n",
       "        [1177.3525,  622.9869, 1222.2386,  667.9508],\n",
       "        [ 523.9020,  306.6059,  568.3209,  351.3579],\n",
       "        [ 709.4512,  291.8343,  754.1907,  337.4313],\n",
       "        [1163.2228,  375.6643, 1208.1484,  420.3381],\n",
       "        [ 376.7430,  584.1204,  422.4892,  629.6350],\n",
       "        [1050.0187,  424.7399, 1095.6641,  470.8136],\n",
       "        [ 128.5336,  953.3351,  173.8523,  998.2373],\n",
       "        [1194.7485,  867.7128, 1238.7955,  911.8247],\n",
       "        [ 618.8249,  942.9947,  664.3034,  988.3636],\n",
       "        [ 681.3127,  584.1384,  727.4300,  628.9984],\n",
       "        [ 682.0151,  671.4707,  726.5549,  716.3178],\n",
       "        [ 363.1434,  953.3409,  408.8951,  998.5701],\n",
       "        [1081.8831,  564.4156, 1126.1177,  609.3875],\n",
       "        [1046.5032,  326.5228, 1091.9852,  372.1740],\n",
       "        [ 672.0932,  997.7241,  717.3352, 1042.3998],\n",
       "        [ 425.3841,  250.9804,  470.0027,  296.2181],\n",
       "        [ 523.9484,  374.4776,  568.5618,  420.5699],\n",
       "        [ 309.4562,  953.6311,  355.5804,  998.2201],\n",
       "        [1177.8029,  564.7988, 1221.8680,  609.3581],\n",
       "        [ 801.3932,  291.6122,  845.8742,  338.1042],\n",
       "        [ 619.4262,  888.8321,  663.5100,  934.2049],\n",
       "        [ 672.1001,  942.9834,  717.6937,  987.9557],\n",
       "        [1082.8564,  679.0481, 1126.3247,  724.2260],\n",
       "        [ 672.7848,  888.9197,  716.8593,  933.9903],\n",
       "        [ 377.1347,  671.6302,  422.2506,  717.5779],\n",
       "        [ 460.0551,  645.9117,  506.6656,  690.6802],\n",
       "        [ 762.8124,  717.2894,  806.9310,  762.4851],\n",
       "        [ 522.6176,  953.5715,  567.4506,  998.4536],\n",
       "        [ 377.4988,  743.4633,  421.6340,  790.4914],\n",
       "        [ 524.2607,  251.7905,  569.1658,  296.8473],\n",
       "        [ 310.3356,  889.1298,  354.8025,  934.3579],\n",
       "        [ 347.9197,  250.7121,  393.1032,  295.8635],\n",
       "        [ 480.2640,  953.5737,  524.6224,  999.1046],\n",
       "        [1111.1901,  425.6036, 1156.5240,  470.1042],\n",
       "        [1178.3289,  678.9739, 1222.1959,  724.2570],\n",
       "        [1114.1712,  326.2967, 1159.2309,  370.8477],\n",
       "        [ 128.8643,  889.0249,  172.9402,  934.1129],\n",
       "        [ 447.3258,  914.3845,  492.3659,  959.5029],\n",
       "        [ 646.1696,   19.6600,  690.3869,   66.2797],\n",
       "        [ 363.5442,  889.2469,  407.9367,  934.7165],\n",
       "        [1082.4942,  744.2167, 1126.5235,  789.4254],\n",
       "        [ 410.2731,  892.2692,  454.2287,  936.6167],\n",
       "        [ 930.5489,  717.2065,  975.5251,  762.4123],\n",
       "        [1178.0819,  743.9814, 1222.2714,  789.8316],\n",
       "        [ 875.4328,  208.0106,  896.3256,  240.7179],\n",
       "        [1163.6737,  273.4493, 1200.6647,  294.3501],\n",
       "        [ 461.1859,  718.3268,  506.0972,  763.1645],\n",
       "        [1014.2412,  525.6035, 1052.7049,  547.6822],\n",
       "        [ 353.8388,  464.7859,  386.6091,  485.9578],\n",
       "        [ 997.4576,  337.7313, 1030.1761,  358.3785],\n",
       "        [ 942.3040,  338.0766,  974.8185,  358.7312],\n",
       "        [1000.9941,  437.1933, 1034.2871,  458.7020],\n",
       "        [ 944.4272,  437.4789,  977.8033,  458.7383],\n",
       "        [1356.9578,  825.1461, 1377.7205,  858.1225],\n",
       "        [ 801.1131,  345.4828,  834.0471,  366.6438],\n",
       "        [ 405.1576,  782.1999,  591.9119,  878.3665],\n",
       "        [ 746.7857,  265.2584,  779.8003,  285.9042],\n",
       "        [ 801.1408,  265.2316,  834.1358,  286.2510],\n",
       "        [ 534.9036,  429.4517,  557.6520,  462.2584],\n",
       "        [ 748.1965,  345.7442,  781.3545,  366.3479],\n",
       "        [1104.4597,  273.9924, 1142.4578,  294.0976],\n",
       "        [ 292.9684,  205.7660,  331.7887,  242.8890],\n",
       "        [ 467.2145,   88.2953,  500.2842,  107.0146],\n",
       "        [ 960.4927, 1113.7840,  999.1155, 1134.2697],\n",
       "        [1534.5039,  271.2139, 1573.8725,  298.1000],\n",
       "        [ 960.5354,  525.9501,  998.8312,  546.0716],\n",
       "        [ 822.4865,  734.9633,  846.5438,  763.9263],\n",
       "        [ 169.4899, 1110.6909,  209.7702, 1137.7461],\n",
       "        [ 233.8600,   62.6881,  411.2427,  130.2350],\n",
       "        [1534.2347,  383.7746, 1574.7354,  411.6257],\n",
       "        [1303.8011,  904.0823, 1320.6555,  933.0853],\n",
       "        [ 203.5383,  781.1559,  330.4440,  888.8661],\n",
       "        [ 904.7980,  734.7901,  928.7530,  763.9248],\n",
       "        [ 819.6933,  694.8760,  850.1500,  706.3626],\n",
       "        [ 868.9071,  719.2019,  892.4769,  749.6700],\n",
       "        [ 901.4850,  688.2839,  931.4131,  699.4134],\n",
       "        [ 866.1385,  688.1417,  896.0527,  699.6478],\n",
       "        [ 907.0241, 1070.9941,  924.1687, 1099.7115],\n",
       "        [1243.6119,  529.5720, 1272.0975,  548.2231],\n",
       "        [ 865.3278,  895.5276,  895.4117,  906.7846],\n",
       "        [ 662.2626,   82.9796,  673.8599,  112.4504],\n",
       "        [ 866.2589, 1145.1886,  884.9749, 1174.7596],\n",
       "        [1534.5581,  876.7693, 1573.7519,  903.5478],\n",
       "        [ 816.1235,  778.9124,  950.1071,  894.9756],\n",
       "        [ 486.2001,  263.9757,  499.8805,  281.5107],\n",
       "        [ 329.7223,  865.5388,  389.9008,  906.2603],\n",
       "        [ 308.9204,  262.1418,  328.5428,  283.6282],\n",
       "        [ 870.8668,  309.0423,  900.1243,  320.7034],\n",
       "        [ 581.1756,  306.4222,  597.0483,  323.6911]], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_boxes(pred_boxes, gt_boxes, iou_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Match predicted bounding boxes to ground truth boxes based on IoU.\n",
    "\n",
    "    Args:\n",
    "        pred_boxes (torch.Tensor): Tensor of shape (N, 4), predicted boxes in [x1, y1, x2, y2] format.\n",
    "        gt_boxes (torch.Tensor): Tensor of shape (M, 4), ground truth boxes in [x1, y1, x2, y2] format.\n",
    "        iou_threshold (float): IoU threshold for matching.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (N,), where each element is the index of the matching ground truth box, or -1 if no match.\n",
    "    \"\"\"\n",
    "    classes = gt_boxes[:, 0]\n",
    "    ious = metrics.box_iou(pred_boxes, gt_boxes[:, 1:])  # (N, M)\n",
    "\n",
    "    # Find the best match for each predicted box\n",
    "    max_iou, max_idx = ious.max(dim=1)  # (N,)\n",
    "\n",
    "    # Assign matches based on IoU threshold\n",
    "    matches = max_idx.clone()\n",
    "    matches[max_iou < iou_threshold] = -1\n",
    "\n",
    "    return matches, ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIDObjectDetector import PIDPreprocessor\n",
    "import pandas as pd\n",
    "\n",
    "labels_path = root / 'Data' / 'test'/ 'fw_system' / 'labels' / '1de1eea3-fw_system-1_cropped.txt'\n",
    "\n",
    "\n",
    "df = pd.read_csv(labels_path, sep=' ', header=None)\n",
    "df.head()\n",
    "\n",
    "preprocessor = PIDPreprocessor(\n",
    "    root=root,\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT,\n",
    "    num_imgs=1,\n",
    "    val_start=1,\n",
    ")\n",
    "\n",
    "df_torch = df.apply(lambda row: preprocessor.convert_yolo_to_torch(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_tensor = torch.tensor(df_torch[['label', 'x1', 'y1', 'x2', 'y2']].values)\n",
    "gt_labels = gt_tensor[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, ious = match_boxes(pred_bboxes, gt_tensor)\n",
    "gt_labels = gt_tensor[:, 0]\n",
    "gt_labels_filtered = gt_labels[results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_classes = torch.tensor([x.id for x in detection_ids])\n",
    "perf = torch.stack((gt_labels_filtered, detection_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (perf[0] == perf[1]).sum()\n",
    "correct_pct = correct / perf.size(1)\n",
    "\n",
    "print(f\"Accuracy: {correct_pct:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_detections(detection_img, height, width, tile_size):\n",
    "        \"\"\"\n",
    "        Pad detection images to 640 x 640 pixels for consistent sizing for YOLO inference\n",
    "        \"\"\"\n",
    "        pad_left = (tile_size - width) // 2\n",
    "        pad_right = tile_size - width - pad_left\n",
    "        pad_top = (tile_size - height) // 2\n",
    "        pad_bottom = tile_size - height - pad_top\n",
    "        \n",
    "        padded_detection = cv2.copyMakeBorder(\n",
    "            detection_img,\n",
    "            pad_top, pad_bottom, pad_left, pad_right,\n",
    "            cv2.BORDER_CONSTANT,\n",
    "            value=[255, 255, 255]  # White padding\n",
    "        )\n",
    "        return padded_detection\n",
    "\n",
    "for i, prediction in enumerate(pred_bboxes):\n",
    "    x1, y1, x2, y2 = prediction.long()\n",
    "    x1, y1, x2, y2 = x1.item(), y1.item(), x2.item(), y2.item()\n",
    "\n",
    "    detection = im[y1:y2, x1:x2]\n",
    "    height, width = (y2 - y1), (x2 - x1)\n",
    "    padded_detection = pad_detections(detection, height, width, tile_size=256)\n",
    "    \n",
    "    write_path = root / 'Data' / 'test' / 'fw_system' / 'predictions' / model_folder / f'prediction_{i}.png'\n",
    "    if not cv2.imwrite(write_path, padded_detection):\n",
    "        print('Failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in gt_imgs:\n",
    "    object = cv2.imread(img)\n",
    "    height, width, _ = object.shape\n",
    "    class_id = re.findall('(\\d+).png', img)[0]\n",
    "    \n",
    "    padded_detection = pad_detections(object, height, width, tile_size=256)\n",
    "\n",
    "    name = d['categories'][int(class_id)]['name']\n",
    "\n",
    "    write_path = root / 'Data' / 'test' / 'fw_system' / 'labels' / f'{name}.png'\n",
    "    if not cv2.imwrite(write_path, padded_detection):\n",
    "        print('Failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = metrics.box_iou(label_tensor_torch.to(device), pred_bboxes.to(device), eps=1e-07)\n",
    "mean_iou, tp, fp, fn = detection_metrics(iou, iou_threshold=IOU_THRESHOLD)\n",
    "print(f\"Mean IOU: {mean_iou:.3f}\")\n",
    "print(f\"Overall Box Recall: {tp / (tp + fn):.3f}\")\n",
    "print(f\"Overall Box Precision: {tp /(tp + fp):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
